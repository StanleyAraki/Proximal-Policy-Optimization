{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function\n",
    "\n",
    "Need to do the following in terminal to run:\n",
    "mkdir tmp\n",
    "mkdir tmp/ppo\n",
    "mkdir plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from ipynb.fs.full.CartPolePPO import Agent\n",
    "from utils import plot_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving models ...\n",
      "episode 0 score 11.0 avg score 11.0 time_steps 11 learning_steps 0\n",
      "... saving models ...\n",
      "episode 1 score 19.0 avg score 15.0 time_steps 30 learning_steps 1\n",
      "episode 2 score 13.0 avg score 14.3 time_steps 43 learning_steps 2\n",
      "episode 3 score 13.0 avg score 14.0 time_steps 56 learning_steps 2\n",
      "episode 4 score 14.0 avg score 14.0 time_steps 70 learning_steps 3\n",
      "episode 5 score 18.0 avg score 14.7 time_steps 88 learning_steps 4\n",
      "... saving models ...\n",
      "episode 6 score 19.0 avg score 15.3 time_steps 107 learning_steps 5\n",
      "episode 7 score 10.0 avg score 14.6 time_steps 117 learning_steps 5\n",
      "episode 8 score 18.0 avg score 15.0 time_steps 135 learning_steps 6\n",
      "... saving models ...\n",
      "episode 9 score 33.0 avg score 16.8 time_steps 168 learning_steps 8\n",
      "... saving models ...\n",
      "episode 10 score 46.0 avg score 19.5 time_steps 214 learning_steps 10\n",
      "... saving models ...\n",
      "episode 11 score 27.0 avg score 20.1 time_steps 241 learning_steps 12\n",
      "... saving models ...\n",
      "episode 12 score 39.0 avg score 21.5 time_steps 280 learning_steps 14\n",
      "... saving models ...\n",
      "episode 13 score 87.0 avg score 26.2 time_steps 367 learning_steps 18\n",
      "... saving models ...\n",
      "episode 14 score 40.0 avg score 27.1 time_steps 407 learning_steps 20\n",
      "... saving models ...\n",
      "episode 15 score 63.0 avg score 29.4 time_steps 470 learning_steps 23\n",
      "episode 16 score 28.0 avg score 29.3 time_steps 498 learning_steps 24\n",
      "episode 17 score 17.0 avg score 28.6 time_steps 515 learning_steps 25\n",
      "episode 18 score 16.0 avg score 27.9 time_steps 531 learning_steps 26\n",
      "episode 19 score 37.0 avg score 28.4 time_steps 568 learning_steps 28\n",
      "... saving models ...\n",
      "episode 20 score 71.0 avg score 30.4 time_steps 639 learning_steps 31\n",
      "... saving models ...\n",
      "episode 21 score 93.0 avg score 33.3 time_steps 732 learning_steps 36\n",
      "... saving models ...\n",
      "episode 22 score 38.0 avg score 33.5 time_steps 770 learning_steps 38\n",
      "... saving models ...\n",
      "episode 23 score 55.0 avg score 34.4 time_steps 825 learning_steps 41\n",
      "... saving models ...\n",
      "episode 24 score 102.0 avg score 37.1 time_steps 927 learning_steps 46\n",
      "episode 25 score 23.0 avg score 36.5 time_steps 950 learning_steps 47\n",
      "episode 26 score 22.0 avg score 36.0 time_steps 972 learning_steps 48\n",
      "episode 27 score 32.0 avg score 35.9 time_steps 1004 learning_steps 50\n",
      "episode 28 score 16.0 avg score 35.2 time_steps 1020 learning_steps 51\n",
      "episode 29 score 34.0 avg score 35.1 time_steps 1054 learning_steps 52\n",
      "episode 30 score 34.0 avg score 35.1 time_steps 1088 learning_steps 54\n",
      "episode 31 score 56.0 avg score 35.8 time_steps 1144 learning_steps 57\n",
      "episode 32 score 31.0 avg score 35.6 time_steps 1175 learning_steps 58\n",
      "episode 33 score 25.0 avg score 35.3 time_steps 1200 learning_steps 60\n",
      "episode 34 score 22.0 avg score 34.9 time_steps 1222 learning_steps 61\n",
      "episode 35 score 14.0 avg score 34.3 time_steps 1236 learning_steps 61\n",
      "episode 36 score 14.0 avg score 33.8 time_steps 1250 learning_steps 62\n",
      "episode 37 score 86.0 avg score 35.2 time_steps 1336 learning_steps 66\n",
      "episode 38 score 76.0 avg score 36.2 time_steps 1412 learning_steps 70\n",
      "... saving models ...\n",
      "episode 39 score 76.0 avg score 37.2 time_steps 1488 learning_steps 74\n",
      "... saving models ...\n",
      "episode 40 score 98.0 avg score 38.7 time_steps 1586 learning_steps 79\n",
      "episode 41 score 38.0 avg score 38.7 time_steps 1624 learning_steps 81\n",
      "... saving models ...\n",
      "episode 42 score 84.0 avg score 39.7 time_steps 1708 learning_steps 85\n",
      "... saving models ...\n",
      "episode 43 score 149.0 avg score 42.2 time_steps 1857 learning_steps 92\n",
      "... saving models ...\n",
      "episode 44 score 93.0 avg score 43.3 time_steps 1950 learning_steps 97\n",
      "... saving models ...\n",
      "episode 45 score 94.0 avg score 44.4 time_steps 2044 learning_steps 102\n",
      "... saving models ...\n",
      "episode 46 score 170.0 avg score 47.1 time_steps 2214 learning_steps 110\n",
      "... saving models ...\n",
      "episode 47 score 87.0 avg score 47.9 time_steps 2301 learning_steps 115\n",
      "... saving models ...\n",
      "episode 48 score 54.0 avg score 48.1 time_steps 2355 learning_steps 117\n",
      "episode 49 score 40.0 avg score 47.9 time_steps 2395 learning_steps 119\n",
      "episode 50 score 17.0 avg score 47.3 time_steps 2412 learning_steps 120\n",
      "episode 51 score 26.0 avg score 46.9 time_steps 2438 learning_steps 121\n",
      "episode 52 score 17.0 avg score 46.3 time_steps 2455 learning_steps 122\n",
      "episode 53 score 14.0 avg score 45.7 time_steps 2469 learning_steps 123\n",
      "episode 54 score 20.0 avg score 45.3 time_steps 2489 learning_steps 124\n",
      "episode 55 score 137.0 avg score 46.9 time_steps 2626 learning_steps 131\n",
      "episode 56 score 78.0 avg score 47.4 time_steps 2704 learning_steps 135\n",
      "episode 57 score 66.0 avg score 47.8 time_steps 2770 learning_steps 138\n",
      "... saving models ...\n",
      "episode 58 score 110.0 avg score 48.8 time_steps 2880 learning_steps 144\n",
      "... saving models ...\n",
      "episode 59 score 115.0 avg score 49.9 time_steps 2995 learning_steps 149\n",
      "... saving models ...\n",
      "episode 60 score 80.0 avg score 50.4 time_steps 3075 learning_steps 153\n",
      "... saving models ...\n",
      "episode 61 score 106.0 avg score 51.3 time_steps 3181 learning_steps 159\n",
      "... saving models ...\n",
      "episode 62 score 131.0 avg score 52.6 time_steps 3312 learning_steps 165\n",
      "... saving models ...\n",
      "episode 63 score 58.0 avg score 52.7 time_steps 3370 learning_steps 168\n",
      "... saving models ...\n",
      "episode 64 score 78.0 avg score 53.0 time_steps 3448 learning_steps 172\n",
      "... saving models ...\n",
      "episode 65 score 82.0 avg score 53.5 time_steps 3530 learning_steps 176\n",
      "... saving models ...\n",
      "episode 66 score 164.0 avg score 55.1 time_steps 3694 learning_steps 184\n",
      "... saving models ...\n",
      "episode 67 score 119.0 avg score 56.1 time_steps 3813 learning_steps 190\n",
      "episode 68 score 43.0 avg score 55.9 time_steps 3856 learning_steps 192\n",
      "episode 69 score 49.0 avg score 55.8 time_steps 3905 learning_steps 195\n",
      "episode 70 score 66.0 avg score 55.9 time_steps 3971 learning_steps 198\n",
      "... saving models ...\n",
      "episode 71 score 71.0 avg score 56.1 time_steps 4042 learning_steps 202\n",
      "episode 72 score 42.0 avg score 55.9 time_steps 4084 learning_steps 204\n",
      "episode 73 score 60.0 avg score 56.0 time_steps 4144 learning_steps 207\n",
      "episode 74 score 27.0 avg score 55.6 time_steps 4171 learning_steps 208\n",
      "episode 75 score 59.0 avg score 55.7 time_steps 4230 learning_steps 211\n",
      "episode 76 score 65.0 avg score 55.8 time_steps 4295 learning_steps 214\n",
      "... saving models ...\n",
      "episode 77 score 105.0 avg score 56.4 time_steps 4400 learning_steps 220\n",
      "... saving models ...\n",
      "episode 78 score 156.0 avg score 57.7 time_steps 4556 learning_steps 227\n",
      "... saving models ...\n",
      "episode 79 score 105.0 avg score 58.3 time_steps 4661 learning_steps 233\n",
      "... saving models ...\n",
      "episode 80 score 116.0 avg score 59.0 time_steps 4777 learning_steps 238\n",
      "... saving models ...\n",
      "episode 81 score 106.0 avg score 59.5 time_steps 4883 learning_steps 244\n",
      "... saving models ...\n",
      "episode 82 score 157.0 avg score 60.7 time_steps 5040 learning_steps 252\n",
      "... saving models ...\n",
      "episode 83 score 186.0 avg score 62.2 time_steps 5226 learning_steps 261\n",
      "... saving models ...\n",
      "episode 84 score 216.0 avg score 64.0 time_steps 5442 learning_steps 272\n",
      "... saving models ...\n",
      "episode 85 score 165.0 avg score 65.2 time_steps 5607 learning_steps 280\n",
      "... saving models ...\n",
      "episode 86 score 272.0 avg score 67.6 time_steps 5879 learning_steps 293\n",
      "... saving models ...\n",
      "episode 87 score 135.0 avg score 68.3 time_steps 6014 learning_steps 300\n",
      "... saving models ...\n",
      "episode 88 score 127.0 avg score 69.0 time_steps 6141 learning_steps 307\n",
      "... saving models ...\n",
      "episode 89 score 101.0 avg score 69.4 time_steps 6242 learning_steps 312\n",
      "episode 90 score 63.0 avg score 69.3 time_steps 6305 learning_steps 315\n",
      "... saving models ...\n",
      "episode 91 score 100.0 avg score 69.6 time_steps 6405 learning_steps 320\n",
      "... saving models ...\n",
      "episode 92 score 275.0 avg score 71.8 time_steps 6680 learning_steps 334\n",
      "... saving models ...\n",
      "episode 93 score 240.0 avg score 73.6 time_steps 6920 learning_steps 346\n",
      "... saving models ...\n",
      "episode 94 score 202.0 avg score 75.0 time_steps 7122 learning_steps 356\n",
      "... saving models ...\n",
      "episode 95 score 247.0 avg score 76.8 time_steps 7369 learning_steps 368\n",
      "... saving models ...\n",
      "episode 96 score 145.0 avg score 77.5 time_steps 7514 learning_steps 375\n",
      "episode 97 score 72.0 avg score 77.4 time_steps 7586 learning_steps 379\n",
      "episode 98 score 39.0 avg score 77.0 time_steps 7625 learning_steps 381\n",
      "episode 99 score 67.0 avg score 76.9 time_steps 7692 learning_steps 384\n",
      "... saving models ...\n",
      "episode 100 score 121.0 avg score 78.0 time_steps 7813 learning_steps 390\n",
      "... saving models ...\n",
      "episode 101 score 204.0 avg score 79.9 time_steps 8017 learning_steps 400\n",
      "... saving models ...\n",
      "episode 102 score 198.0 avg score 81.7 time_steps 8215 learning_steps 410\n",
      "... saving models ...\n",
      "episode 103 score 178.0 avg score 83.4 time_steps 8393 learning_steps 419\n",
      "... saving models ...\n",
      "episode 104 score 108.0 avg score 84.3 time_steps 8501 learning_steps 425\n",
      "... saving models ...\n",
      "episode 105 score 120.0 avg score 85.3 time_steps 8621 learning_steps 431\n",
      "... saving models ...\n",
      "episode 106 score 238.0 avg score 87.5 time_steps 8859 learning_steps 442\n",
      "... saving models ...\n",
      "episode 107 score 89.0 avg score 88.3 time_steps 8948 learning_steps 447\n",
      "... saving models ...\n",
      "episode 108 score 176.0 avg score 89.9 time_steps 9124 learning_steps 456\n",
      "... saving models ...\n",
      "episode 109 score 139.0 avg score 91.0 time_steps 9263 learning_steps 463\n",
      "... saving models ...\n",
      "episode 110 score 89.0 avg score 91.4 time_steps 9352 learning_steps 467\n",
      "... saving models ...\n",
      "episode 111 score 158.0 avg score 92.7 time_steps 9510 learning_steps 475\n",
      "... saving models ...\n",
      "episode 112 score 64.0 avg score 92.9 time_steps 9574 learning_steps 478\n",
      "episode 113 score 78.0 avg score 92.8 time_steps 9652 learning_steps 482\n",
      "episode 114 score 39.0 avg score 92.8 time_steps 9691 learning_steps 484\n",
      "... saving models ...\n",
      "episode 115 score 115.0 avg score 93.4 time_steps 9806 learning_steps 490\n",
      "... saving models ...\n",
      "episode 116 score 219.0 avg score 95.3 time_steps 10025 learning_steps 501\n",
      "... saving models ...\n",
      "episode 117 score 138.0 avg score 96.5 time_steps 10163 learning_steps 508\n",
      "... saving models ...\n",
      "episode 118 score 106.0 avg score 97.4 time_steps 10269 learning_steps 513\n",
      "... saving models ...\n",
      "episode 119 score 356.0 avg score 100.6 time_steps 10625 learning_steps 531\n",
      "... saving models ...\n",
      "episode 120 score 79.0 avg score 100.7 time_steps 10704 learning_steps 535\n",
      "... saving models ...\n",
      "episode 121 score 235.0 avg score 102.1 time_steps 10939 learning_steps 546\n",
      "... saving models ...\n",
      "episode 122 score 449.0 avg score 106.2 time_steps 11388 learning_steps 569\n",
      "... saving models ...\n",
      "episode 123 score 500.0 avg score 110.6 time_steps 11888 learning_steps 594\n",
      "... saving models ...\n",
      "episode 124 score 252.0 avg score 112.1 time_steps 12140 learning_steps 607\n",
      "... saving models ...\n",
      "episode 125 score 295.0 avg score 114.8 time_steps 12435 learning_steps 621\n",
      "... saving models ...\n",
      "episode 126 score 500.0 avg score 119.6 time_steps 12935 learning_steps 646\n",
      "... saving models ...\n",
      "episode 127 score 466.0 avg score 124.0 time_steps 13401 learning_steps 670\n",
      "... saving models ...\n",
      "episode 128 score 500.0 avg score 128.8 time_steps 13901 learning_steps 695\n",
      "... saving models ...\n",
      "episode 129 score 500.0 avg score 133.5 time_steps 14401 learning_steps 720\n",
      "... saving models ...\n",
      "episode 130 score 500.0 avg score 138.1 time_steps 14901 learning_steps 745\n",
      "... saving models ...\n",
      "episode 131 score 500.0 avg score 142.6 time_steps 15401 learning_steps 770\n",
      "... saving models ...\n",
      "episode 132 score 500.0 avg score 147.3 time_steps 15901 learning_steps 795\n",
      "... saving models ...\n",
      "episode 133 score 333.0 avg score 150.3 time_steps 16234 learning_steps 811\n",
      "... saving models ...\n",
      "episode 134 score 198.0 avg score 152.1 time_steps 16432 learning_steps 821\n",
      "... saving models ...\n",
      "episode 135 score 172.0 avg score 153.7 time_steps 16604 learning_steps 830\n",
      "... saving models ...\n",
      "episode 136 score 346.0 avg score 157.0 time_steps 16950 learning_steps 847\n",
      "... saving models ...\n",
      "episode 137 score 500.0 avg score 161.1 time_steps 17450 learning_steps 872\n",
      "... saving models ...\n",
      "episode 138 score 500.0 avg score 165.4 time_steps 17950 learning_steps 897\n",
      "... saving models ...\n",
      "episode 139 score 500.0 avg score 169.6 time_steps 18450 learning_steps 922\n",
      "... saving models ...\n",
      "episode 140 score 433.0 avg score 173.0 time_steps 18883 learning_steps 944\n",
      "... saving models ...\n",
      "episode 141 score 330.0 avg score 175.9 time_steps 19213 learning_steps 960\n",
      "... saving models ...\n",
      "episode 142 score 285.0 avg score 177.9 time_steps 19498 learning_steps 974\n",
      "... saving models ...\n",
      "episode 143 score 500.0 avg score 181.4 time_steps 19998 learning_steps 999\n",
      "... saving models ...\n",
      "episode 144 score 500.0 avg score 185.5 time_steps 20498 learning_steps 1024\n",
      "... saving models ...\n",
      "episode 145 score 500.0 avg score 189.5 time_steps 20998 learning_steps 1049\n",
      "... saving models ...\n",
      "episode 146 score 347.0 avg score 191.3 time_steps 21345 learning_steps 1067\n",
      "... saving models ...\n",
      "episode 147 score 185.0 avg score 192.3 time_steps 21530 learning_steps 1076\n",
      "episode 148 score 25.0 avg score 192.0 time_steps 21555 learning_steps 1077\n",
      "... saving models ...\n",
      "episode 149 score 167.0 avg score 193.3 time_steps 21722 learning_steps 1086\n",
      "... saving models ...\n",
      "episode 150 score 196.0 avg score 195.1 time_steps 21918 learning_steps 1095\n",
      "... saving models ...\n",
      "episode 151 score 250.0 avg score 197.3 time_steps 22168 learning_steps 1108\n",
      "... saving models ...\n",
      "episode 152 score 217.0 avg score 199.3 time_steps 22385 learning_steps 1119\n",
      "... saving models ...\n",
      "episode 153 score 264.0 avg score 201.8 time_steps 22649 learning_steps 1132\n",
      "... saving models ...\n",
      "episode 154 score 299.0 avg score 204.6 time_steps 22948 learning_steps 1147\n",
      "... saving models ...\n",
      "episode 155 score 304.0 avg score 206.3 time_steps 23252 learning_steps 1162\n",
      "... saving models ...\n",
      "episode 156 score 500.0 avg score 210.5 time_steps 23752 learning_steps 1187\n",
      "... saving models ...\n",
      "episode 157 score 362.0 avg score 213.4 time_steps 24114 learning_steps 1205\n",
      "... saving models ...\n",
      "episode 158 score 310.0 avg score 215.4 time_steps 24424 learning_steps 1221\n",
      "... saving models ...\n",
      "episode 159 score 500.0 avg score 219.3 time_steps 24924 learning_steps 1246\n",
      "... saving models ...\n",
      "episode 160 score 500.0 avg score 223.5 time_steps 25424 learning_steps 1271\n",
      "... saving models ...\n",
      "episode 161 score 343.0 avg score 225.9 time_steps 25767 learning_steps 1288\n",
      "... saving models ...\n",
      "episode 162 score 500.0 avg score 229.6 time_steps 26267 learning_steps 1313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPoleMain.ipynb Cell 3'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPoleMain.ipynb#ch0000003?line=29'>30</a>\u001b[0m agent\u001b[39m.\u001b[39mremember(observation, action, prob, val, reward, done)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPoleMain.ipynb#ch0000003?line=30'>31</a>\u001b[0m \u001b[39mif\u001b[39;00m n_steps \u001b[39m%\u001b[39m N \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# if true, it's time to perform learning function\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPoleMain.ipynb#ch0000003?line=31'>32</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPoleMain.ipynb#ch0000003?line=32'>33</a>\u001b[0m     learn_iters \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPoleMain.ipynb#ch0000003?line=33'>34</a>\u001b[0m observation \u001b[39m=\u001b[39m observation_\n",
      "File \u001b[0;32m~/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb:267\u001b[0m, in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=0'>1</a>\u001b[0m {\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=1'>2</a>\u001b[0m  \u001b[39m\"\u001b[39m\u001b[39mcells\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=2'>3</a>\u001b[0m   {\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=3'>4</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=4'>5</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=5'>6</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m# Pre-Implementation Notes\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=8'>9</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Memory indices = [0, 1, 2, ..., 19]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Batches start at multiples of batch_size[0,5,10,15]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=10'>11</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Shuffle memorie sthen take batch size chunks\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=11'>12</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Two distinct networks instead of shared inputs\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=12'>13</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Critic evalutes states (not (state, action) pairs)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=13'>14</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Actor decides what to do based on current state\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=14'>15</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Network outputs probabilities(softmax) for a distribution\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=15'>16</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Exploration due to nature of distribution (probabilistic and set up so that each element has finite probability)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=16'>17</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Memory is fixed to length T(20) steps\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=17'>18</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Track states, actions, rewards, dones(terminal flags), values, log probs\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=18'>19</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Shuffle memories and sample batches (size = 5)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=19'>20</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Perform 4 epochs of updates on each batch\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=20'>21</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Update rule for Actor is complex\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=21'>22</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - $L^\u001b[39m\u001b[39m{CPI}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtheta) = \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mhat\u001b[39m\u001b[39m{E}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{t}\u001b[39;00m\u001b[39m[\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mfrac\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mpi_\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtheta(a_t | s_t)}\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mpi_\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtheta old}(a_t | s_t)}\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mhat\u001b[39m\u001b[39m{A}\u001b[39;00m\u001b[39m_t] = \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mhat\u001b[39m\u001b[39m{E}\u001b[39;00m\u001b[39m_t[r_t(\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtheta)\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mhat\u001b[39m\u001b[39m{A_t}\u001b[39;00m\u001b[39m]$\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=22'>23</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Based on ratio of new policy to old (can use logs)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=23'>24</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Also takes into account the advantage ($A_t$)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=24'>25</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Define epsilon (~0.2) for clip/min operations\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=25'>26</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - $L^\u001b[39m\u001b[39m{CLIP}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtheta) = \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mhat\u001b[39m\u001b[39m{E}\u001b[39;00m\u001b[39m_t[min(r_t(\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtheta)\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mhat\u001b[39m\u001b[39m{A}\u001b[39;00m\u001b[39m_t), clip(r_t(\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtheta), 1-\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mepsilon, 1+\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mepsilon)\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mhat\u001b[39m\u001b[39m{A}\u001b[39;00m\u001b[39m_t]$\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=26'>27</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - We will be using this for the loss of the actor network\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=27'>28</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Pessimistic lower bound to the loss (smaller loss, smaller gradient, smaller update)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=28'>29</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Advantage at each time step:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=29'>30</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - $\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mhat\u001b[39m\u001b[39m{A}\u001b[39;00m\u001b[39m_t = \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mdelta_t + (\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mgamma\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mlambda)\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mdelta_\u001b[39m\u001b[39m{\u001b[39m\u001b[39mt+1} + ... + ... + (\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mgamma\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mlambda)^\u001b[39m\u001b[39m{\u001b[39m\u001b[39mT-t+1}\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mdelta_\u001b[39m\u001b[39m{\u001b[39m\u001b[39mT-1}$, where $\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mdelta_t = r_t + \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mgamma V(s_\u001b[39m\u001b[39m{\u001b[39m\u001b[39mt+1})-V(s_t)$\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=30'>31</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Tells us the benefit of the new state over the old (equal to the sum of $\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mdelta_t$)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=31'>32</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        - $\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mdelta_t$ is the reward at a time step + difference in the estimated value of the new and old state\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=32'>33</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - $\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mgamma$ is the output of the critic network \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=33'>34</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - $\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mlambda$ is a smoothing parameter that helps us lower variance (~0.95)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=34'>35</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Using nested for loops\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=35'>36</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Critic loss:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=36'>37</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Return = advantage + critic value (from memory)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=37'>38</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - $L_\u001b[39m\u001b[39m{critic}\u001b[39;00m\u001b[39m$ = MSE(return - critic value (from network))\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=38'>39</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- Total loss is sum of clipped actor and critic:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=39'>40</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - $L_t^\u001b[39m\u001b[39m{\u001b[39m\u001b[39mCLIP + VF + S}(\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtheta) = \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mhat\u001b[39m\u001b[39m{E}\u001b[39;00m\u001b[39m_t[L_t^\u001b[39m\u001b[39m{CLIP}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtheta) - c_1L_t^\u001b[39m\u001b[39m{VF}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtheta) + c_2S[\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mpi\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtheta](s_t)] $\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=40'>41</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - We are doing gradient *ascent*, not gradient *descent*\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=41'>42</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - $c_1$ = 0.5 (for the loss of the critic)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=42'>43</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=43'>44</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- **Things we won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt implement**\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=44'>45</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Coupled actor critic entropy term (S)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=45'>46</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Can also be used for continuous actions\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=46'>47</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt do multi core CPU implementation $\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mrightarrow$ GPU\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=47'>48</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=48'>49</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m- **Data Structures we will need**\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=49'>50</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Class for replay buffers $\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mrightarrow$ lists\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=50'>51</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Class for actor network, class for critic network\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=51'>52</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Class for agent (ties everything together)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=52'>53</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    - Main loop to train and evaluate\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=53'>54</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=54'>55</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mMade possible by https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/PolicyGradient/PPO/torch\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=55'>56</a>\u001b[0m    ]\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=56'>57</a>\u001b[0m   },\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=57'>58</a>\u001b[0m   {\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=58'>59</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=59'>60</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=60'>61</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=61'>62</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=62'>63</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=63'>64</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport os\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=64'>65</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport numpy as np\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=65'>66</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport torch as T\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=66'>67</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport torch.nn.functional as F\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=67'>68</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport torch.nn as nn\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=68'>69</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport torch.optim as optim\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=69'>70</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfrom torch.distributions.categorical import Categorical\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=70'>71</a>\u001b[0m    ]\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=71'>72</a>\u001b[0m   },\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=72'>73</a>\u001b[0m   {\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=73'>74</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=74'>75</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=75'>76</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=76'>77</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m# PPO Memory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=77'>78</a>\u001b[0m    ]\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=78'>79</a>\u001b[0m   },\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=79'>80</a>\u001b[0m   {\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=80'>81</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=81'>82</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=82'>83</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=83'>84</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=84'>85</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=85'>86</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mclass PPOMemory:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=86'>87</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def __init__(self, batch_size):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=87'>88</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.states = []  # states encounted\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=88'>89</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.probs = []  # log probs\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=89'>90</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.vals = []  # value critic calculates\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=90'>91</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actions = []  # actions we actually took\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=91'>92</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.rewards = []  # rewards received\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=92'>93</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.dones = []  # terminal flags\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=93'>94</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=94'>95</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.batch_size = batch_size\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=95'>96</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=96'>97</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def generate_batches(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=97'>98</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # Have a list of intergesr that correspond to indices of memories, and have batch size chuncks of those memories\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=98'>99</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # Shuffle up those indices and take batch size chuncks of those indices\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=99'>100</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        num_states = len(self.states)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=100'>101</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        batch_start = np.arange(0, num_states, self.batch_size)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=101'>102</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        indices = np.arange(num_states, dtype=np.int64)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=102'>103</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # shuffle those indices to handle stochatic part of the minibatch of stochastic gradienst ascent\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=103'>104</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        np.random.shuffle(indices)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=104'>105</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # Takes all possible starting points of batches and goes from those indices to i + batch_size\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=105'>106</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        batches = [indices[i:i+self.batch_size] for i in batch_start]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=106'>107</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=107'>108</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        return np.array(self.states), np.array(self.actions), np.array(self.probs), np.array(self.vals), np.array(self.rewards), np.array(self.dones), batches\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=108'>109</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=109'>110</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def store_memory(self, state, action, probs, vals, reward, done):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=110'>111</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.states.append(state)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=111'>112</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.probs.append(probs)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=112'>113</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actions.append(action)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=113'>114</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.vals.append(vals)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=114'>115</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.rewards.append(reward)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=115'>116</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.dones.append(done)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=116'>117</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=117'>118</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def clear_memory(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=118'>119</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.states = []\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=119'>120</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.probs = []\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=120'>121</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.vals = []\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=121'>122</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actions = []\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=122'>123</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.rewards = []\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=123'>124</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.dones = []\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=124'>125</a>\u001b[0m    ]\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=125'>126</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=126'>127</a>\u001b[0m   {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=127'>128</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=128'>129</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=129'>130</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=130'>131</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m# ActorNetwork\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=131'>132</a>\u001b[0m    ]\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=132'>133</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=133'>134</a>\u001b[0m   {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=134'>135</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=135'>136</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m3\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=136'>137</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=137'>138</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=138'>139</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=139'>140</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mclass ActorNetwork(nn.Module):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=140'>141</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def __init__(self, n_actions, input_dims, alpha,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=141'>142</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            fc1_dims=256, fc2_dims=256, chkpt_dir=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtmp/ppo\u001b[39m\u001b[39m'\u001b[39m\u001b[39m):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=142'>143</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        super(ActorNetwork, self).__init__()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=143'>144</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=144'>145</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.checkpoint_file = os.path.join(chkpt_dir, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mactor_torch_ppo_CartPole\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=145'>146</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actor = nn.Sequential(\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=146'>147</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Linear(*input_dims, fc1_dims), # Need to manually pass in input\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=147'>148</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.ReLU(),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=148'>149</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Linear(fc1_dims, fc2_dims),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=149'>150</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.ReLU(),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=150'>151</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Linear(fc2_dims, n_actions),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=151'>152</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Softmax(dim=-1) # SoftMax takes care of the fact that we\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre dealing with probabilities and they sum to 1\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=152'>153</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        )\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=153'>154</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=154'>155</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=155'>156</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # self.device = T.device(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m\u001b[39m if T.cuda.is_available() else \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=156'>157</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.device = T.device(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=157'>158</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.to(self.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=158'>159</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=159'>160</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def forward(self, state):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=160'>161</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        dist = self.actor(state)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=161'>162</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        dist = Categorical(dist)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=162'>163</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # Calculating series of probabilities that we will use to get our actual actions\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=163'>164</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # We can use that to get the log probabilities for the calculation of the ratio, for the two probabilities of our learning function\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=164'>165</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=165'>166</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        return dist\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=166'>167</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=167'>168</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def save_checkpoint(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=168'>169</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        T.save(self.state_dict(), self.checkpoint_file)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=169'>170</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=170'>171</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def load_checkpoint(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=171'>172</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.load_state_dict(T.load(self.checkpoint_file))\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=172'>173</a>\u001b[0m    ]\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=173'>174</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=174'>175</a>\u001b[0m   {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=175'>176</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=176'>177</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=177'>178</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=178'>179</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m# CriticNetwork Class\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=179'>180</a>\u001b[0m    ]\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=180'>181</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=181'>182</a>\u001b[0m   {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=182'>183</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=183'>184</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=184'>185</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=185'>186</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=186'>187</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=187'>188</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mclass CriticNetwork(nn.Module):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=188'>189</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=189'>190</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            chkpt_dir=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtmp/ppo\u001b[39m\u001b[39m'\u001b[39m\u001b[39m):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=190'>191</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        super(CriticNetwork, self).__init__()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=191'>192</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=192'>193</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.checkpoint_file = os.path.join(chkpt_dir, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcritic_torch_ppo_CartPole\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=193'>194</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.critic = nn.Sequential(\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=194'>195</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Linear(*input_dims, fc1_dims), # Need to manually pass in inputs\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=195'>196</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.ReLU(),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=196'>197</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Linear(fc1_dims, fc2_dims),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=197'>198</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.ReLU(),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=198'>199</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Linear(fc2_dims, 1)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=199'>200</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        ) # handles batch size automatically\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=200'>201</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=201'>202</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=202'>203</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # self.device = T.device(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m\u001b[39m xif T.cuda.is_available() else \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=203'>204</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.device = T.device(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=204'>205</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.to(self.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=205'>206</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=206'>207</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def forward(self, state):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=207'>208</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        value = self.critic(state)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=208'>209</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=209'>210</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        return value\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=210'>211</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=211'>212</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def save_checkpoint(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=212'>213</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        T.save(self.state_dict(), self.checkpoint_file)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=213'>214</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=214'>215</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def load_checkpoint(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=215'>216</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.load_state_dict(T.load(self.checkpoint_file))\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=216'>217</a>\u001b[0m    ]\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=217'>218</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=218'>219</a>\u001b[0m   {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=219'>220</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=220'>221</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=221'>222</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=222'>223</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m# Agent Class\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=223'>224</a>\u001b[0m    ]\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=224'>225</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=225'>226</a>\u001b[0m   {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=226'>227</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=227'>228</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m5\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=228'>229</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=229'>230</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=230'>231</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=231'>232</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mclass Agent:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=232'>233</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def __init__(self, num_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95, policy_clip = 0.2, batch_size=64, N=2048, num_epochs=10): # N is horizon, number of steps before update\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=233'>234</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.gamma = gamma\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=234'>235</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.policy_clip = policy_clip\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=235'>236</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.num_epochs = num_epochs\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=236'>237</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.gae_lambda = gae_lambda\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=237'>238</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actor = ActorNetwork(num_actions, input_dims, alpha)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=238'>239</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.critic = CriticNetwork(input_dims, alpha)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=239'>240</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.memory = PPOMemory(batch_size)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=240'>241</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=241'>242</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def remember(self, state, action, probs, vals, reward, done):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=242'>243</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.memory.store_memory(state, action, probs, vals, reward, done)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=243'>244</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=244'>245</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def save_models(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=245'>246</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m... saving models ...\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=246'>247</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actor.save_checkpoint()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=247'>248</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.critic.save_checkpoint()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=248'>249</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=249'>250</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def load_models(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=250'>251</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m... loading models ...\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=251'>252</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actor.load_checkpoint()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=252'>253</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.critic.load_checkpoint()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=253'>254</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=254'>255</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def choose_action(self, observation):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=255'>256</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # handle choosing an action that takes an observation of the current state of the environment, converts to torch tensor\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=256'>257</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        state = T.tensor([observation], dtype=T.float)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=257'>258</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # print(\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mTYPE: \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m, state.type())\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=258'>259</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # state = T.tensor([observation], dtype=T.float).to(self.actor.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=259'>260</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        state = state.to(self.actor.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=260'>261</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=261'>262</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        dist = self.actor(state)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=262'>263</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        value = self.critic(state)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=263'>264</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        action = dist.sample()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=264'>265</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=265'>266</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        probs = T.squeeze(dist.log_prob(action)).item()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=266'>267</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        action = T.squeeze(action).item()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=267'>268</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        value = T.squeeze(value).item()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=268'>269</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=269'>270</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        return action, probs, value\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=270'>271</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=271'>272</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def learn(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=272'>273</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # our learning function\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=273'>274</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        for _ in range(self.num_epochs):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=274'>275</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            state_arr, action_arr, old_probs_arr, vals_arr, reward_arr, dones_arr, batches = self.memory.generate_batches()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=275'>276</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=276'>277</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            values = vals_arr\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=277'>278</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            # Start calculating advantages\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=278'>279</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            advantage = np.zeros(len(reward_arr), dtype=np.float32)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=279'>280</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            for t in range(len(reward_arr)-1):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=280'>281</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                discount = 1\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=281'>282</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                advantage_time = 0\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=282'>283</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                for k in range(t, len(reward_arr)-1):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=283'>284</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                    # delta_t part is in the paranthesis\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=284'>285</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                    advantage_time += discount *(reward_arr[k] + self.gamma * values[k+1] * (1-int(dones_arr[k])) - values[k])\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=285'>286</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                    discount *= self.gamma * self.gae_lambda # takes care of multiplicative factor (gamma lambda) ^ (T-t+1)delta_T-1\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=286'>287</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                advantage[t] = advantage_time\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=287'>288</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            advantage = T.tensor(advantage).to(self.actor.device) \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=288'>289</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=289'>290</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            values = T.tensor(values).to(self.actor.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=290'>291</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            for batch in batches:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=291'>292</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=292'>293</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                old_probs = T.tensor(old_probs_arr[batch]).to(self.actor.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=293'>294</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                actions = T.tensor(action_arr[batch]).to(self.actor.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=294'>295</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                # we have the bottom of the numerator now(pi old)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=295'>296</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=296'>297</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                # we now need pi new\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=297'>298</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                dist = self.actor(states)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=298'>299</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                critic_value = self.critic(states)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=299'>300</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                critic_value = T.squeeze(critic_value)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=300'>301</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=301'>302</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                new_probs = dist.log_prob(actions)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=302'>303</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                prob_ratio = new_probs.exp() / old_probs.exp()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=303'>304</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                weighted_probs = advantage[batch] * prob_ratio\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=304'>305</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip) * advantage[batch]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=305'>306</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=306'>307</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=307'>308</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                returns = advantage[batch] + values[batch]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=308'>309</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                critic_loss = (returns-critic_value) ** 2\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=309'>310</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                critic_loss = critic_loss.mean()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=310'>311</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=311'>312</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                total_loss = actor_loss + 0.5*critic_loss\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=312'>313</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                self.actor.optimizer.zero_grad() # zero the gradient\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=313'>314</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                self.critic.optimizer.zero_grad() # zero the gradient\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=314'>315</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                total_loss.backward() # backpropagate total loss\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=315'>316</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                self.actor.optimizer.step() # upstep optimizer\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=316'>317</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                self.critic.optimizer.step() # upstep optimizer\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=317'>318</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.memory.clear_memory()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=318'>319</a>\u001b[0m    ]\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=319'>320</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=320'>321</a>\u001b[0m   {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=321'>322</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=322'>323</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: null,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=323'>324</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=324'>325</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=325'>326</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: []\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=326'>327</a>\u001b[0m   }\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=327'>328</a>\u001b[0m  ],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=328'>329</a>\u001b[0m  \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=329'>330</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39minterpreter\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=330'>331</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mhash\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39me96d89988d0d8e7d7a8ab5719ad00aeab7b060c61a49b19476af80724aec9e8a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=331'>332</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=332'>333</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mkernelspec\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=333'>334</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mdisplay_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mPython 3.9.7 (\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=334'>335</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mlanguage\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=335'>336</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpython3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=336'>337</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=337'>338</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mlanguage_info\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=338'>339</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcodemirror_mode\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=339'>340</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mipython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=340'>341</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m3\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=341'>342</a>\u001b[0m    },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=342'>343</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mfile_extension\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=343'>344</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmimetype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtext/x-python\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=344'>345</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=345'>346</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mnbconvert_exporter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=346'>347</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mpygments_lexer\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mipython3\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=347'>348</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m3.9.7\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=348'>349</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=349'>350</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39morig_nbformat\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=350'>351</a>\u001b[0m  },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=351'>352</a>\u001b[0m  \u001b[39m\"\u001b[39m\u001b[39mnbformat\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=352'>353</a>\u001b[0m  \u001b[39m\"\u001b[39m\u001b[39mnbformat_minor\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/Data_Analysis_Portfolio/Proximal-Policy-Optimization/CartPolePPO.ipynb?line=353'>354</a>\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=137'>138</a>\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=138'>139</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=140'>141</a>\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=141'>142</a>\u001b[0m            grads,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=142'>143</a>\u001b[0m            exp_avgs,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=143'>144</a>\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=144'>145</a>\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=145'>146</a>\u001b[0m            state_steps,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=146'>147</a>\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=147'>148</a>\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=148'>149</a>\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=149'>150</a>\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=150'>151</a>\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=151'>152</a>\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=152'>153</a>\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=153'>154</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py:105\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py?line=102'>103</a>\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py?line=103'>104</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py?line=104'>105</a>\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py?line=108'>109</a>\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py?line=109'>110</a>\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v1')\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "\n",
    "    agent = Agent(num_actions = env.action_space.n, batch_size = 5, alpha = 0.0003, num_epochs = 4, input_dims = env.observation_space.shape)\n",
    "    n_games = 300\n",
    "\n",
    "    figure_file = 'plots/CartPole.png'\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    for i in range(n_games): # 103 min 8.8 sec for 300 iterations\n",
    "        observation = env.reset()\n",
    "        # observation = prepro(observation)\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0: # if true, it's time to perform learning function\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score: # if best score found\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n",
    "\n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "                'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_learning_curve(x, score_history, figure_file, \"Training Episodes\", \"Average Scores\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving models ...\n"
     ]
    }
   ],
   "source": [
    "# agent.save_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trained Model and Run Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading models ...\n"
     ]
    }
   ],
   "source": [
    "# Might be good idea to export and backup the models that perform well so we don't need to re-train it again from scratch. \n",
    "agent.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "score = 0\n",
    "done = False\n",
    "n_steps = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    action, prob, val = agent.choose_action(observation)\n",
    "    observation_, reward, done, info = env.step(action)\n",
    "    n_steps += 1\n",
    "    score += reward\n",
    "    agent.remember(observation, action, prob, val, reward, done)\n",
    "    if n_steps % N == 0: # if true, it's time to perform learning function\n",
    "        agent.learn()\n",
    "        learn_iters += 1\n",
    "    observation = observation_\n",
    "print(score)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e96d89988d0d8e7d7a8ab5719ad00aeab7b060c61a49b19476af80724aec9e8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
