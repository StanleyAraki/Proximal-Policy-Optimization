{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from ipynb.fs.full.BreakoutPPO import Agent\n",
    "from utils import plot_learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Breakout Environment for Faster Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image(Code from class)\n",
    "def prepro(image):\n",
    "    image = image[35:195]  # crop\n",
    "    image = image[::2, ::2, 0]  # downsample by factor of 2\n",
    "    image[image == 144] = 0  # erase background (background type 1)\n",
    "    image[image == 109] = 0  # erase background (background type 2)\n",
    "    image[image != 0] = 1  # everything else (paddles, ball) just set to 1\n",
    "    return np.reshape(image, (1, 80, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Breakout Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n",
      "/Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/BreakoutPPO.ipynb:161: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1646756029501/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  \"        value = F.relu(self.conv3(value))\\n\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading models ...\n",
      "... saving models ...\n",
      "episode 0 score 0.0 avg score 0.0 time_steps 181 learning_steps 9\n",
      "episode 1 score 0.0 avg score 0.0 time_steps 354 learning_steps 17\n",
      "episode 2 score 0.0 avg score 0.0 time_steps 568 learning_steps 28\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # env = gym.make('BreakoutDeterministic-v4', render_mode = 'human')\n",
    "    env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.00025 # learning rate / epsilon value I think\n",
    "\n",
    "    raw_image = env.reset()\n",
    "    preprocessed_image = prepro(raw_image)  # (1, 80, 80)\n",
    "\n",
    "    flattened = preprocessed_image.flatten()\n",
    "    agent = Agent(num_actions=env.action_space.n, batch_size=batch_size,\n",
    "                  alpha=alpha, num_epochs=n_epochs, input_dims=preprocessed_image.shape)\n",
    "    n_games = 15  # 45 mins for 100 iterations of training \n",
    "\n",
    "    figure_file = 'plots/Breakout_Conv.png'\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    # Load model\n",
    "    agent.load_models()\n",
    "\n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            observation = prepro(observation)  # need to preprocess each time\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            if action == 2 or action == 3:\n",
    "                print(action)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            # observation = prepro(observation)\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0:  # if true, it's time to perform learning function\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:  # if best score found\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n",
    "\n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "              'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_learning_curve(x, score_history, figure_file,\n",
    "                        \"Training Episodes\", \"Average Scores\", \"Breakout\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing/Rendering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/Breakout.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/Breakout.ipynb#ch0000006?line=0'>1</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mBreakoutDeterministic-v4\u001b[39m\u001b[39m'\u001b[39m, render_mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/Breakout.ipynb#ch0000006?line=1'>2</a>\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/Breakout.ipynb#ch0000006?line=2'>3</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "    env = gym.make('BreakoutDeterministic-v4', render_mode = 'human')\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003 # learning rate / epsilon value I think\n",
    "\n",
    "    raw_image = env.reset()\n",
    "    preprocessed_image = prepro(raw_image)  # (1, 80, 80)\n",
    "\n",
    "    flattened = preprocessed_image.flatten()\n",
    "    agent = Agent(num_actions=env.action_space.n, batch_size=5,\n",
    "                  alpha=0.0003, num_epochs=4, input_dims=preprocessed_image.shape)\n",
    "    n_games = 1  # 4 mins for 100 iterations of training. Score does not improve though...\n",
    "\n",
    "    figure_file = 'plots/Breakout_Conv.png' \n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    # Load model\n",
    "    agent.load_models()\n",
    "\n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            observation = prepro(observation)  # need to preprocess each time\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            if action != 1:\n",
    "                print(action)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0:  # if true, it's time to perform learning function\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:  # if best score found\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n",
    "\n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "              'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e96d89988d0d8e7d7a8ab5719ad00aeab7b060c61a49b19476af80724aec9e8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
