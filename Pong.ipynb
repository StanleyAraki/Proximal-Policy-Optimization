{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from ipynb.fs.full.PongPPO import Agent\n",
    "from utils import plot_learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Pong Environment for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image(Code from class)\n",
    "def prepro(image):\n",
    "    image = image[35:195]  # crop\n",
    "    image = image[::2, ::2, 0]  # downsample by factor of 2\n",
    "    image[image == 144] = 0  # erase background (background type 1)\n",
    "    image[image == 109] = 0  # erase background (background type 2)\n",
    "    image[image != 0] = 1  # everything else (paddles, ball) just set to 1\n",
    "    return np.reshape(image, (1, 80, 80))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Pong Environment\n",
    "\n",
    "If it works on Pong environment, then it will likely work on the Breakout Environment as well (with a few tweaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading models ...\n",
      "... saving models ...\n",
      "episode 0 score -21.0 avg score -21.0 time_steps 765 learning_steps 38\n",
      "episode 1 score -21.0 avg score -21.0 time_steps 1530 learning_steps 76\n",
      "episode 2 score -21.0 avg score -21.0 time_steps 2295 learning_steps 114\n",
      "episode 3 score -21.0 avg score -21.0 time_steps 3060 learning_steps 153\n",
      "episode 4 score -21.0 avg score -21.0 time_steps 3825 learning_steps 191\n",
      "episode 5 score -21.0 avg score -21.0 time_steps 4590 learning_steps 229\n",
      "episode 6 score -21.0 avg score -21.0 time_steps 5355 learning_steps 267\n",
      "episode 7 score -21.0 avg score -21.0 time_steps 6120 learning_steps 306\n",
      "episode 8 score -21.0 avg score -21.0 time_steps 6885 learning_steps 344\n",
      "episode 9 score -21.0 avg score -21.0 time_steps 7650 learning_steps 382\n",
      "episode 10 score -21.0 avg score -21.0 time_steps 8415 learning_steps 420\n",
      "episode 11 score -21.0 avg score -21.0 time_steps 9180 learning_steps 459\n",
      "episode 12 score -21.0 avg score -21.0 time_steps 9945 learning_steps 497\n",
      "episode 13 score -21.0 avg score -21.0 time_steps 10710 learning_steps 535\n",
      "episode 14 score -21.0 avg score -21.0 time_steps 11475 learning_steps 573\n",
      "episode 15 score -21.0 avg score -21.0 time_steps 12240 learning_steps 612\n",
      "episode 16 score -21.0 avg score -21.0 time_steps 13005 learning_steps 650\n",
      "episode 17 score -21.0 avg score -21.0 time_steps 13770 learning_steps 688\n",
      "episode 18 score -21.0 avg score -21.0 time_steps 14535 learning_steps 726\n",
      "episode 19 score -21.0 avg score -21.0 time_steps 15300 learning_steps 765\n",
      "episode 20 score -21.0 avg score -21.0 time_steps 16065 learning_steps 803\n",
      "episode 21 score -21.0 avg score -21.0 time_steps 16830 learning_steps 841\n",
      "episode 22 score -21.0 avg score -21.0 time_steps 17595 learning_steps 879\n",
      "episode 23 score -21.0 avg score -21.0 time_steps 18360 learning_steps 918\n",
      "episode 24 score -21.0 avg score -21.0 time_steps 19125 learning_steps 956\n",
      "episode 25 score -21.0 avg score -21.0 time_steps 19890 learning_steps 994\n",
      "episode 26 score -21.0 avg score -21.0 time_steps 20655 learning_steps 1032\n",
      "episode 27 score -21.0 avg score -21.0 time_steps 21420 learning_steps 1071\n",
      "episode 28 score -21.0 avg score -21.0 time_steps 22185 learning_steps 1109\n",
      "episode 29 score -21.0 avg score -21.0 time_steps 22950 learning_steps 1147\n",
      "episode 30 score -21.0 avg score -21.0 time_steps 23715 learning_steps 1185\n",
      "episode 31 score -21.0 avg score -21.0 time_steps 24480 learning_steps 1224\n",
      "episode 32 score -21.0 avg score -21.0 time_steps 25273 learning_steps 1263\n",
      "episode 33 score -21.0 avg score -21.0 time_steps 26160 learning_steps 1308\n",
      "episode 34 score -21.0 avg score -21.0 time_steps 26985 learning_steps 1349\n",
      "episode 35 score -21.0 avg score -21.0 time_steps 27810 learning_steps 1390\n",
      "... saving models ...\n",
      "episode 36 score -20.0 avg score -21.0 time_steps 28653 learning_steps 1432\n",
      "episode 37 score -21.0 avg score -21.0 time_steps 29437 learning_steps 1471\n",
      "episode 38 score -21.0 avg score -21.0 time_steps 30221 learning_steps 1511\n",
      "episode 39 score -21.0 avg score -21.0 time_steps 31033 learning_steps 1551\n",
      "... saving models ...\n",
      "episode 40 score -20.0 avg score -21.0 time_steps 31979 learning_steps 1598\n",
      "... saving models ...\n",
      "episode 41 score -20.0 avg score -20.9 time_steps 32823 learning_steps 1641\n",
      "episode 42 score -21.0 avg score -20.9 time_steps 33588 learning_steps 1679\n",
      "episode 43 score -21.0 avg score -20.9 time_steps 34353 learning_steps 1717\n",
      "episode 44 score -21.0 avg score -20.9 time_steps 35118 learning_steps 1755\n",
      "episode 45 score -21.0 avg score -20.9 time_steps 35883 learning_steps 1794\n",
      "episode 46 score -21.0 avg score -20.9 time_steps 36648 learning_steps 1832\n",
      "episode 47 score -21.0 avg score -20.9 time_steps 37413 learning_steps 1870\n",
      "episode 48 score -21.0 avg score -20.9 time_steps 38178 learning_steps 1908\n",
      "episode 49 score -21.0 avg score -20.9 time_steps 38943 learning_steps 1947\n",
      "episode 50 score -21.0 avg score -20.9 time_steps 39708 learning_steps 1985\n",
      "episode 51 score -21.0 avg score -20.9 time_steps 40473 learning_steps 2023\n",
      "episode 52 score -21.0 avg score -20.9 time_steps 41238 learning_steps 2061\n",
      "episode 53 score -21.0 avg score -20.9 time_steps 42003 learning_steps 2100\n",
      "episode 54 score -21.0 avg score -20.9 time_steps 42796 learning_steps 2139\n",
      "episode 55 score -21.0 avg score -20.9 time_steps 43589 learning_steps 2179\n",
      "episode 56 score -21.0 avg score -20.9 time_steps 44373 learning_steps 2218\n",
      "episode 57 score -21.0 avg score -20.9 time_steps 45157 learning_steps 2257\n",
      "episode 58 score -21.0 avg score -20.9 time_steps 45941 learning_steps 2297\n",
      "episode 59 score -21.0 avg score -20.9 time_steps 46762 learning_steps 2338\n",
      "episode 60 score -21.0 avg score -21.0 time_steps 47555 learning_steps 2377\n",
      "episode 61 score -21.0 avg score -21.0 time_steps 48480 learning_steps 2424\n",
      "episode 62 score -21.0 avg score -21.0 time_steps 49245 learning_steps 2462\n",
      "episode 63 score -21.0 avg score -21.0 time_steps 50010 learning_steps 2500\n",
      "episode 64 score -21.0 avg score -21.0 time_steps 50775 learning_steps 2538\n",
      "episode 65 score -21.0 avg score -21.0 time_steps 51540 learning_steps 2577\n",
      "episode 66 score -21.0 avg score -21.0 time_steps 52305 learning_steps 2615\n",
      "episode 67 score -21.0 avg score -21.0 time_steps 53070 learning_steps 2653\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/Pong.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/Pong.ipynb#ch0000005?line=36'>37</a>\u001b[0m agent\u001b[39m.\u001b[39mremember(observation, action, prob, val, reward, done)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/Pong.ipynb#ch0000005?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m n_steps \u001b[39m%\u001b[39m N \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# if true, it's time to perform learning function\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/Pong.ipynb#ch0000005?line=38'>39</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/Pong.ipynb#ch0000005?line=39'>40</a>\u001b[0m     learn_iters \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/Pong.ipynb#ch0000005?line=40'>41</a>\u001b[0m observation \u001b[39m=\u001b[39m observation_\n",
      "File \u001b[0;32m~/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb:219\u001b[0m, in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=0'>1</a>\u001b[0m {\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=1'>2</a>\u001b[0m  \u001b[39m\"\u001b[39m\u001b[39mcells\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=2'>3</a>\u001b[0m   {\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=3'>4</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=4'>5</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=5'>6</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m# Pong\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=7'>8</a>\u001b[0m    ]\n\u001b[1;32m      <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=8'>9</a>\u001b[0m   },\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=9'>10</a>\u001b[0m   {\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=10'>11</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=11'>12</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=12'>13</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=13'>14</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=14'>15</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=15'>16</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport os\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=16'>17</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport numpy as np\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=17'>18</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport torch as T\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=18'>19</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport torch.nn.functional as F\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=19'>20</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport torch.nn as nn\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=20'>21</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport torch.optim as optim\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=21'>22</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfrom torch.distributions.categorical import Categorical\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=22'>23</a>\u001b[0m    ]\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=23'>24</a>\u001b[0m   },\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=24'>25</a>\u001b[0m   {\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=25'>26</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=26'>27</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=27'>28</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=28'>29</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m# PPO Memory Class\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=29'>30</a>\u001b[0m    ]\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=30'>31</a>\u001b[0m   },\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=31'>32</a>\u001b[0m   {\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=32'>33</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=33'>34</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=34'>35</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=35'>36</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=36'>37</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=37'>38</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mclass PPOMemory:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=38'>39</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def __init__(self, batch_size):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=39'>40</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.states = []  # states encounted\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=40'>41</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.probs = []  # log probs\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=41'>42</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.vals = []  # value critic calculates\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=42'>43</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actions = []  # actions we actually took\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=43'>44</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.rewards = []  # rewards received\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=44'>45</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.dones = []  # terminal flags\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=45'>46</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=46'>47</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.batch_size = batch_size\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=47'>48</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=48'>49</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def generate_batches(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=49'>50</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # Have a list of intergesr that correspond to indices of memories, and have batch size chuncks of those memories\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=50'>51</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # Shuffle up those indices and take batch size chuncks of those indices\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=51'>52</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        num_states = len(self.states)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=52'>53</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        batch_start = np.arange(0, num_states, self.batch_size)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=53'>54</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        indices = np.arange(num_states, dtype=np.int64)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=54'>55</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # shuffle those indices to handle stochatic part of the minibatch of stochastic gradienst ascent\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=55'>56</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        np.random.shuffle(indices)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=56'>57</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # Takes all possible starting points of batches and goes from those indices to i + batch_size\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=57'>58</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        batches = [indices[i:i+self.batch_size] for i in batch_start]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=58'>59</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=59'>60</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        return np.array(self.states), np.array(self.actions), np.array(self.probs), np.array(self.vals), np.array(self.rewards), np.array(self.dones), batches\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=60'>61</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=61'>62</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def store_memory(self, state, action, probs, vals, reward, done):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=62'>63</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.states.append(state)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=63'>64</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.probs.append(probs)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=64'>65</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actions.append(action)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=65'>66</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.vals.append(vals)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=66'>67</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.rewards.append(reward)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=67'>68</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.dones.append(done)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=68'>69</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=69'>70</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def clear_memory(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=70'>71</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.states = []\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=71'>72</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.probs = []\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=72'>73</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.vals = []\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=73'>74</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actions = []\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=74'>75</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.rewards = []\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=75'>76</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.dones = []\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=76'>77</a>\u001b[0m    ]\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=77'>78</a>\u001b[0m   },\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=78'>79</a>\u001b[0m   {\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=79'>80</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=80'>81</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=81'>82</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=82'>83</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m# ActorNetwork Class\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=83'>84</a>\u001b[0m    ]\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=84'>85</a>\u001b[0m   },\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=85'>86</a>\u001b[0m   {\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=86'>87</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=87'>88</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m3\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=88'>89</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=89'>90</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=90'>91</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=91'>92</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mclass ActorNetwork(nn.Module):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=92'>93</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def __init__(self, n_actions, input_dims, alpha,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=93'>94</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            fc1_dims=256, fc2_dims=256, chkpt_dir=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtmp/ppo\u001b[39m\u001b[39m'\u001b[39m\u001b[39m):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=94'>95</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        super(ActorNetwork, self).__init__()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=95'>96</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=96'>97</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.checkpoint_file = os.path.join(chkpt_dir, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mactor_torch_ppo_Pong\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=97'>98</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actor = nn.Sequential(\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=98'>99</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Linear(*input_dims, fc1_dims), # Need to manually pass in input\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=99'>100</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.ReLU(),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=100'>101</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Linear(fc1_dims, fc2_dims),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=101'>102</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.ReLU(),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=102'>103</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Linear(fc2_dims, n_actions),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=103'>104</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Softmax(dim=-1) # SoftMax takes care of the fact that we\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre dealing with probabilities and they sum to 1\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=104'>105</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        )\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=105'>106</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=106'>107</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=107'>108</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # self.device = T.device(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m\u001b[39m if T.cuda.is_available() else \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=108'>109</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.device = T.device(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=109'>110</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.to(self.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=110'>111</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=111'>112</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def forward(self, state):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=112'>113</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        dist = self.actor(state)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=113'>114</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        dist = Categorical(dist)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=114'>115</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # Calculating series of probabilities that we will use to get our actual actions\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=115'>116</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # We can use that to get the log probabilities for the calculation of the ratio, for the two probabilities of our learning function\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=116'>117</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=117'>118</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        return dist\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=118'>119</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=119'>120</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def save_checkpoint(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=120'>121</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        T.save(self.state_dict(), self.checkpoint_file)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=121'>122</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=122'>123</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def load_checkpoint(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=123'>124</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.load_state_dict(T.load(self.checkpoint_file))\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=124'>125</a>\u001b[0m    ]\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=125'>126</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=126'>127</a>\u001b[0m   {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=127'>128</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=128'>129</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=129'>130</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=130'>131</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m# CriticNetwork Class\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=131'>132</a>\u001b[0m    ]\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=132'>133</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=133'>134</a>\u001b[0m   {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=134'>135</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=135'>136</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=136'>137</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=137'>138</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=138'>139</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=139'>140</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mclass CriticNetwork(nn.Module):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=140'>141</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=141'>142</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            chkpt_dir=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtmp/ppo\u001b[39m\u001b[39m'\u001b[39m\u001b[39m):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=142'>143</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        super(CriticNetwork, self).__init__()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=143'>144</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=144'>145</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.checkpoint_file = os.path.join(chkpt_dir, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcritic_torch_ppo_Pong\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=145'>146</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.critic = nn.Sequential(\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=146'>147</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Linear(*input_dims, fc1_dims), # Need to manually pass in inputs\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=147'>148</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.ReLU(),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=148'>149</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Linear(fc1_dims, fc2_dims),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=149'>150</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.ReLU(),\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=150'>151</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                nn.Linear(fc2_dims, 1)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=151'>152</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        ) # handles batch size automatically\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=152'>153</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=153'>154</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=154'>155</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # self.device = T.device(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m\u001b[39m xif T.cuda.is_available() else \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=155'>156</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.device = T.device(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=156'>157</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.to(self.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=157'>158</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=158'>159</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def forward(self, state):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=159'>160</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        value = self.critic(state)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=160'>161</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=161'>162</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        return value\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=162'>163</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=163'>164</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def save_checkpoint(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=164'>165</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        T.save(self.state_dict(), self.checkpoint_file)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=165'>166</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=166'>167</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def load_checkpoint(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=167'>168</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.load_state_dict(T.load(self.checkpoint_file))\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=168'>169</a>\u001b[0m    ]\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=169'>170</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=170'>171</a>\u001b[0m   {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=171'>172</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=172'>173</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=173'>174</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=174'>175</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m# Agent Class\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=175'>176</a>\u001b[0m    ]\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=176'>177</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=177'>178</a>\u001b[0m   {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=178'>179</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=179'>180</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m5\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=180'>181</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=181'>182</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=182'>183</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=183'>184</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mclass Agent:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=184'>185</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def __init__(self, num_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95, policy_clip = 0.2, batch_size=64, N=2048, num_epochs=10): # N is horizon, number of steps before update\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=185'>186</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.gamma = gamma\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=186'>187</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.policy_clip = policy_clip\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=187'>188</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.num_epochs = num_epochs\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=188'>189</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.gae_lambda = gae_lambda\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=189'>190</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actor = ActorNetwork(num_actions, input_dims, alpha)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=190'>191</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.critic = CriticNetwork(input_dims, alpha)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=191'>192</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.memory = PPOMemory(batch_size)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=192'>193</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=193'>194</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def remember(self, state, action, probs, vals, reward, done):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=194'>195</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.memory.store_memory(state, action, probs, vals, reward, done)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=195'>196</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=196'>197</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def save_models(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=197'>198</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m... saving models ...\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=198'>199</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actor.save_checkpoint()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=199'>200</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.critic.save_checkpoint()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=200'>201</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=201'>202</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def load_models(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=202'>203</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m... loading models ...\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=203'>204</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.actor.load_checkpoint()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=204'>205</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.critic.load_checkpoint()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=205'>206</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=206'>207</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def choose_action(self, observation):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=207'>208</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # handle choosing an action that takes an observation of the current state of the environment, converts to torch tensor\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=208'>209</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        state = T.tensor([observation.flatten()], dtype=T.float) # Need to flatten the observation from (1, 80, 80) to (1, 6400)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=209'>210</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # print(\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mTYPE: \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m, state.type())\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=210'>211</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # state = T.tensor([observation], dtype=T.float).to(self.actor.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=211'>212</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        state = state.to(self.actor.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=212'>213</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=213'>214</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        dist = self.actor(state)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=214'>215</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        value = self.critic(state)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=215'>216</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        action = dist.sample()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=216'>217</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=217'>218</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        probs = T.squeeze(dist.log_prob(action)).item()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=218'>219</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        action = T.squeeze(action).item()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=219'>220</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        value = T.squeeze(value).item()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=220'>221</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=221'>222</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        return action, probs, value\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=222'>223</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=223'>224</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    def learn(self):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=224'>225</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        # our learning function\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=225'>226</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        for _ in range(self.num_epochs):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=226'>227</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            state_arr, action_arr, old_probs_arr, vals_arr, reward_arr, dones_arr, batches = self.memory.generate_batches()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=227'>228</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=228'>229</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            values = vals_arr\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=229'>230</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            # Start calculating advantages\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=230'>231</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            advantage = np.zeros(len(reward_arr), dtype=np.float32)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=231'>232</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            for t in range(len(reward_arr)-1):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=232'>233</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                discount = 1\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=233'>234</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                advantage_time = 0\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=234'>235</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                for k in range(t, len(reward_arr)-1):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=235'>236</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                    # delta_t part is in the paranthesis\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=236'>237</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                    advantage_time += discount *(reward_arr[k] + self.gamma * values[k+1] * (1-int(dones_arr[k])) - values[k])\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=237'>238</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                    discount *= self.gamma * self.gae_lambda # takes care of multiplicative factor (gamma lambda) ^ (T-t+1)delta_T-1\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=238'>239</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                advantage[t] = advantage_time\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=239'>240</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            advantage = T.tensor(advantage).to(self.actor.device) \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=240'>241</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=241'>242</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            values = T.tensor(values).to(self.actor.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=242'>243</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            for batch in batches:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=243'>244</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                states = T.tensor(state_arr[batch].reshape(5, 6400), dtype=T.float).to(self.actor.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=244'>245</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                old_probs = T.tensor(old_probs_arr[batch]).to(self.actor.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=245'>246</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                actions = T.tensor(action_arr[batch]).to(self.actor.device)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=246'>247</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                # we have the bottom of the numerator now(pi old)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=247'>248</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=248'>249</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                # we now need pi new\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=249'>250</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                dist = self.actor(states)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=250'>251</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                critic_value = self.critic(states)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=251'>252</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                critic_value = T.squeeze(critic_value)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=252'>253</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=253'>254</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                new_probs = dist.log_prob(actions)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=254'>255</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                prob_ratio = new_probs.exp() / old_probs.exp()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=255'>256</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                weighted_probs = advantage[batch] * prob_ratio\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=256'>257</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip) * advantage[batch]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=257'>258</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=258'>259</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=259'>260</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                returns = advantage[batch] + values[batch]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=260'>261</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                critic_loss = (returns-critic_value) ** 2\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=261'>262</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                critic_loss = critic_loss.mean()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=262'>263</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=263'>264</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                total_loss = actor_loss + 0.5*critic_loss\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=264'>265</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                self.actor.optimizer.zero_grad() # zero the gradient\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=265'>266</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                self.critic.optimizer.zero_grad() # zero the gradient\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=266'>267</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                total_loss.backward() # backpropagate total loss\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=267'>268</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                self.actor.optimizer.step() # upstep optimizer\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=268'>269</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m                self.critic.optimizer.step() # upstep optimizer\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=269'>270</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        self.memory.clear_memory()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=270'>271</a>\u001b[0m    ]\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=271'>272</a>\u001b[0m   }\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=272'>273</a>\u001b[0m  ],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=273'>274</a>\u001b[0m  \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=274'>275</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39minterpreter\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=275'>276</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mhash\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39me96d89988d0d8e7d7a8ab5719ad00aeab7b060c61a49b19476af80724aec9e8a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=276'>277</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=277'>278</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mkernelspec\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=278'>279</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mdisplay_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mPython 3.9.7 (\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=279'>280</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mlanguage\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=280'>281</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpython3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=281'>282</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=282'>283</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mlanguage_info\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=283'>284</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcodemirror_mode\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=284'>285</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mipython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=285'>286</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m3\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=286'>287</a>\u001b[0m    },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=287'>288</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mfile_extension\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=288'>289</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmimetype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtext/x-python\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=289'>290</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=290'>291</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mnbconvert_exporter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=291'>292</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mpygments_lexer\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mipython3\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=292'>293</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m3.9.7\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=293'>294</a>\u001b[0m   },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=294'>295</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39morig_nbformat\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=295'>296</a>\u001b[0m  },\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=296'>297</a>\u001b[0m  \u001b[39m\"\u001b[39m\u001b[39mnbformat\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=297'>298</a>\u001b[0m  \u001b[39m\"\u001b[39m\u001b[39mnbformat_minor\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/PongPPO.ipynb?line=298'>299</a>\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=137'>138</a>\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=138'>139</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=140'>141</a>\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=141'>142</a>\u001b[0m            grads,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=142'>143</a>\u001b[0m            exp_avgs,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=143'>144</a>\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=144'>145</a>\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=145'>146</a>\u001b[0m            state_steps,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=146'>147</a>\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=147'>148</a>\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=148'>149</a>\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=149'>150</a>\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=150'>151</a>\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=151'>152</a>\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=152'>153</a>\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py?line=153'>154</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py:105\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py?line=102'>103</a>\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py?line=103'>104</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py?line=104'>105</a>\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py?line=108'>109</a>\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[1;32m    <a href='file:///Users/stanleyaraki/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py?line=109'>110</a>\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('ALE/Pong-v5')\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "\n",
    "    raw_image = env.reset()\n",
    "    preprocessed_image = prepro(raw_image) #(1, 80, 80)\n",
    "\n",
    "    flattened = preprocessed_image.flatten()\n",
    "    agent = Agent(num_actions = env.action_space.n, batch_size = 5, alpha = 0.0003, num_epochs = 4, input_dims = flattened.shape)\n",
    "    n_games = 1300\n",
    "\n",
    "    figure_file = 'plots/Pong.png'\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    # Load model\n",
    "    agent.load_models()\n",
    "    \n",
    "    for i in range(n_games): # 103 min 8.8 sec for 300 iterations\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            observation = prepro(observation) # need to preprocess each time\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0: # if true, it's time to perform learning function\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score: # if best score found\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n",
    "\n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "                'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_learning_curve(x, score_history, figure_file, \"Training Episodes\", \"Average Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    env = gym.make('ALE/Pong-v5')\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "\n",
    "    raw_image = env.reset()\n",
    "    preprocessed_image = prepro(raw_image)  # (1, 80, 80)\n",
    "\n",
    "    flattened = preprocessed_image.flatten()\n",
    "    agent = Agent(num_actions=env.action_space.n, batch_size=5,\n",
    "                alpha=0.0003, num_epochs=4, input_dims=flattened.shape)\n",
    "    n_games = 50\n",
    "\n",
    "    figure_file = 'plots/Pong.png'\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:  # if true, it's time to perform learning function\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    print(score)\n",
    "    env.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e96d89988d0d8e7d7a8ab5719ad00aeab7b060c61a49b19476af80724aec9e8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
