comment: ''
commit: 98813db9e2099cc80568a79ad3d9031a18a2c590
commit_message: Added normalizing layers
is_dirty: true
load_run: null
name: multi_PPO
notes: ''
python_file: /Users/stanleyaraki/Desktop/PPO_389/Proximal-Policy-Optimization/multi_PPO.py
repo_remotes:
- https://github.com/StanleyAraki/Proximal-Policy-Optimization.git
start_step: 0
tags:
- PPO
- multi
trial_date: '2022-05-04'
trial_time: 02:18:22
uuid: ffd43b88cb7111ecb3f4acde48001122
