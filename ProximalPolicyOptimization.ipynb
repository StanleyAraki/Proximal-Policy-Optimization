{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applications of Proximal Policy Optimization Algorithm on Atari Games**\n",
    "\n",
    "Rosaline Zhu, Stanley Araki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries to use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# from ray import tune\n",
    "# from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "# ray.shutdown()\n",
    "# ray.init(include_webui=False, ignore_reinit_error=True)\n",
    "\n",
    "# ENV = \"Breakout-ramDeterministic-v4\"\n",
    "# TARGET_REWARD = 200\n",
    "# TRAINER = DQNTrainer\n",
    "\n",
    "# tune.run(\n",
    "#     TRAINER,\n",
    "#     stop={\"episode_reward_mean\": TARGET_REWARD},\n",
    "#     config={\n",
    "#       \"env\": ENV,\n",
    "#       \"monitor\": True,\n",
    "#       \"evaluation_num_episodes\": 25,\n",
    "#       \"double_q\": True,\n",
    "#       \"hiddens\": [128],\n",
    "#       \"num_workers\": 0,\n",
    "#       \"num_gpus\": 1,\n",
    "#       \"target_network_update_freq\": 12_000,\n",
    "#       \"lr\": 5E-6,\n",
    "#       \"adam_epsilon\": 1E-5,\n",
    "#       \"learning_starts\": 150_000,\n",
    "#       \"buffer_size\": 1_500_000,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up environment for Atari Game**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.utils.play import play, PlayPlot\n",
    "\n",
    "def callback(obs_t, obs_tp1, action, rew, done, info):\n",
    "    return [rew]\n",
    "\n",
    "plotter = PlayPlot(callback, 30 * 5, [\"reward\"])\n",
    "env = gym.make(\"Breakout-ramNoFrameskip-v4\")\n",
    "play(env, callback=plotter.callback, zoom=4)\n",
    "\n",
    "# Finally something that works\n",
    "\n",
    "# env = gym.make('CartPole-v1')\n",
    "# for i_episode in range(20):\n",
    "#     observation = env.reset()\n",
    "#     for t in range(100):\n",
    "#         env.render()\n",
    "#         print(observation)\n",
    "#         action = env.action_space.sample()\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         if done:\n",
    "#             print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "#             break\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up Proximal Policy Optimization Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e96d89988d0d8e7d7a8ab5719ad00aeab7b060c61a49b19476af80724aec9e8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
